{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup\n",
    "*Beautiful Soup*  is a Python library for pulling data out of HTML and XML files. It can be used to scrape data, text, links, or image urls from within a website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing, import `BeautifulSoup`, as well as `requests` for loading websites and `re` for using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, re, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use `requests.get` to load our website. In this example we are interested in downloading articles from the Associated Press *Politics* page.\n",
    "<br>\n",
    "Next, we use `BeautifulSoup` to grab and print the html content of the site. This will be messy at first, but we will learn to look through the html for the desired links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.get(\"https://www.theguardian.com/uk/uksecurity?page=1\") \n",
    "soup = BeautifulSoup(s.content)\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping html for links\n",
    "One common task is extracting all the URLs found within a pageâ€™s < a > tags. The link itself is often labeled href as can be seen in the following code:\n",
    "    \n",
    "`<a class=\"Component-headline-0-2-110\" data-key=\"card-headline\" href=\"/article/election-2020-virus-outbreak-joe-biden-campaigns-kamala-harris-aa0bb12aca5568d20a7d6b86f24da7d0\">`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all(\"a\", {\"href\": True})\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output above still contains all of the < a > tag data (titles, authours, as well as links), the following will sort out ONLY the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in links:\n",
    "    print(o.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just printing the links, let's `append` them to a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list =[]\n",
    "for o in links:\n",
    "    link_list.append(o.attrs[\"href\"])\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a list containing all of our links, but it also contains some unwanted links (to AP social media, etc.)\n",
    "\n",
    "The next cell will run a trick called [list comprehension](https://docs.python.org/3/tutorial/datastructures.html) to only keep those list elements that contain the string `\"/article\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list=[x for x in link_list if \"/2020/\" in x]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we see many duplicate/triplicate links, but we can use a quick trick to eliminate dupes. `Lists` can contain duplicates, but the `dictionary` data type cannot. So the following cell simply turns `new_list` into a dictionary then back into a list, thus eliminating dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = list(dict.fromkeys(new_list))\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_list))\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop brings everthing together, downloading each link, scraping the body text, scrubbing with regex and combining into one long string (this will take a couple minutes to process all articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = ''\n",
    "for link in new_list:\n",
    "    \n",
    "#make whole url from apnews + list element\n",
    "    url = link \n",
    "    print(url)\n",
    "    \n",
    "#get html\n",
    "    s = requests.get(url) \n",
    "    soup = BeautifulSoup(s.content)\n",
    "\n",
    "#grab the article\n",
    "    text = soup.find_all(\"div\", {\"class\": \"content__article-body from-content-api js-article__body\"})\n",
    "    text_str = str(text)\n",
    "\n",
    "#regex    \n",
    "    text_str = re.sub(r'<[^>]+>', ' ', text_str)\n",
    "    text_str = text_str.replace('\\n', '')\n",
    "    text_str = re.sub(r' Topics.*', '', text_str)\n",
    "    \n",
    "#collect all texts\n",
    "    collect = collect + text_str\n",
    "    time.sleep(1)\n",
    "print(collect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the word count of that long string of combined articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(collect.split())) + \" appx words\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can write the complete collected text to a singe .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"guardian_terror1.txt\", \"w\")\n",
    "n = text_file.write(collect)\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
